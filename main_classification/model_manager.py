"""
ModelPool - Qu·∫£n l√Ω pool of STT models v·ªõi thread-safe v√† load balancing
Model Pooling (3-4 copies) ‚Üí Balance memory vs performance ‚Üí Tr√°nh bottleneck
"""

import threading
import logging
import torch
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
from typing import Optional, Tuple, List
import time
from queue import Queue

logger = logging.getLogger(__name__)

class ModelPool:
    """
    Singleton class ƒë·ªÉ qu·∫£n l√Ω pool of STT models

    V·ªõi 30 GSM instances:
    - 1 model: Bottleneck nghi√™m tr·ªçng
    - 4 models: C√¢n b·∫±ng t·ªët (30/4 = 7.5 threads/model)
    - Ti·∫øt ki·ªám: 86% RAM (4 vs 30 models)
    """

    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        """Kh·ªüi t·∫°o singleton instance"""
        if self._initialized:
            return

        self._initialized = True
        self._pool_size = 4  # Default pool size
        self._device = "cuda" if torch.cuda.is_available() else "cpu"
        self._model_id = "nguyenvulebinh/wav2vec2-base-vietnamese-250h"

        # Pool of models
        self._processor: Optional[Wav2Vec2Processor] = None  # Shared processor (nh·∫π)
        self._model_pool: List[Wav2Vec2ForCTC] = []  # Pool of models
        self._model_queue: Queue = Queue()  # Queue ƒë·ªÉ qu·∫£n l√Ω available models
        self._pool_lock = threading.Lock()
        self._loading = False
        self._loaded = False

        # Statistics
        self._total_requests = 0
        self._total_wait_time = 0.0

        logger.info(f"ü§ñ ModelPool initialized:")
        logger.info(f"   - Pool size: {self._pool_size} models")
        logger.info(f"   - Device: {self._device}")
        logger.info(f"   - Model: {self._model_id}")
    
    def set_pool_size(self, pool_size: int):
        """Set pool size (ch·ªâ ƒë∆∞·ª£c g·ªçi tr∆∞·ªõc khi load models)"""
        if self._model_pool:
            raise RuntimeError("Kh√¥ng th·ªÉ thay ƒë·ªïi pool_size sau khi ƒë√£ load models")
        self._pool_size = pool_size
        logger.info(f"üìä Pool size set to: {pool_size}")
    
    def _load_pool(self):
        """Load pool of models (lazy loading, thread-safe)"""
        with self._pool_lock:
            # Double-check locking
            if self._loaded:
                return

            if self._loading:
                # ƒê·ª£i thread kh√°c load xong
                logger.info("‚è≥ Waiting for model pool to be loaded...")
                while self._loading:
                    time.sleep(0.1)
                return

            # B·∫Øt ƒë·∫ßu load pool
            self._loading = True
            try:
                logger.info(f"ü§ñ Loading model pool ({self._pool_size} models)...")

                # Load processor (shared, ch·ªâ 1 l·∫ßn)
                logger.info("üì• Loading processor...")
                self._processor = Wav2Vec2Processor.from_pretrained(self._model_id)
                logger.info("‚úÖ Processor loaded")

                # Load pool of models
                for i in range(self._pool_size):
                    logger.info(f"üì• Loading model {i+1}/{self._pool_size}...")
                    model = Wav2Vec2ForCTC.from_pretrained(self._model_id).to(self._device)
                    self._model_pool.append(model)
                    self._model_queue.put(model)  # Add to available queue
                    logger.info(f"‚úÖ Model {i+1}/{self._pool_size} loaded")

                self._loaded = True
                logger.info(f"üéâ Model pool loaded successfully!")
                logger.info(f"   - {self._pool_size} models on {self._device}")
                logger.info(f"   - Ready to serve {self._pool_size * 7} concurrent requests efficiently")

            except Exception as e:
                logger.error(f"‚ùå Failed to load model pool: {e}")
                raise
            finally:
                self._loading = False

    def get_model(self) -> Tuple[Wav2Vec2Processor, Wav2Vec2ForCTC, str]:
        """
        L·∫•y model t·ª´ pool (blocking n·∫øu pool ƒë·∫ßy)

        Returns:
            Tuple[processor, model, device]

        Note:
            - Ph·∫£i g·ªçi release_model() sau khi d√πng xong
            - S·ª≠ d·ª•ng context manager ƒë·ªÉ t·ª± ƒë·ªông release
        """
        # Lazy load pool n·∫øu ch∆∞a load
        if not self._loaded:
            self._load_pool()

        # L·∫•y model t·ª´ queue (blocking n·∫øu kh√¥ng c√≥ model available)
        start_wait = time.time()
        model = self._model_queue.get()  # Block until a model is available
        wait_time = time.time() - start_wait

        # Update statistics
        self._total_requests += 1
        self._total_wait_time += wait_time

        if wait_time > 0.1:  # Log n·∫øu ph·∫£i ƒë·ª£i l√¢u
            logger.debug(f"‚è≥ Waited {wait_time:.3f}s for model (queue was full)")

        return self._processor, model, self._device

    def release_model(self, model: Wav2Vec2ForCTC):
        """
        Tr·∫£ model v·ªÅ pool sau khi d√πng xong

        Args:
            model: Model c·∫ßn tr·∫£ v·ªÅ pool
        """
        self._model_queue.put(model)
    
    def is_loaded(self) -> bool:
        """Ki·ªÉm tra xem pool ƒë√£ ƒë∆∞·ª£c load ch∆∞a"""
        return self._loaded

    def get_device(self) -> str:
        """L·∫•y device ƒëang s·ª≠ d·ª•ng"""
        return self._device

    def get_pool_size(self) -> int:
        """L·∫•y k√≠ch th∆∞·ªõc pool"""
        return self._pool_size

    def get_statistics(self) -> dict:
        """L·∫•y th·ªëng k√™ s·ª≠ d·ª•ng pool"""
        avg_wait = self._total_wait_time / self._total_requests if self._total_requests > 0 else 0
        return {
            "pool_size": self._pool_size,
            "total_requests": self._total_requests,
            "total_wait_time": self._total_wait_time,
            "avg_wait_time": avg_wait,
            "available_models": self._model_queue.qsize(),
            "busy_models": self._pool_size - self._model_queue.qsize()
        }

    def unload_pool(self):
        """Gi·∫£i ph√≥ng to√†n b·ªô pool kh·ªèi memory (n·∫øu c·∫ßn)"""
        with self._pool_lock:
            # Clear models
            for model in self._model_pool:
                del model
            self._model_pool.clear()

            # Clear queue
            while not self._model_queue.empty():
                try:
                    self._model_queue.get_nowait()
                except:
                    break

            # Clear processor
            if self._processor is not None:
                del self._processor
                self._processor = None

            # Clear CUDA cache n·∫øu ƒëang d√πng GPU
            if self._device == "cuda":
                torch.cuda.empty_cache()

            self._loaded = False
            logger.info("üóëÔ∏è Model pool unloaded from memory")


class ModelContext:
    """
    Context manager ƒë·ªÉ t·ª± ƒë·ªông release model sau khi d√πng

    Usage:
        with ModelContext() as (processor, model, device):
            # Use model here
            ...
        # Model t·ª± ƒë·ªông ƒë∆∞·ª£c release
    """

    def __init__(self, pool: ModelPool):
        self.pool = pool
        self.model = None
        self.processor = None
        self.device = None

    def __enter__(self):
        self.processor, self.model, self.device = self.pool.get_model()
        return self.processor, self.model, self.device

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.model is not None:
            self.pool.release_model(self.model)
        return False


# Singleton instance
model_manager = ModelPool()
model_manager.set_pool_size(4)  # 4 models cho 30 instances

def get_model_context():
    """Helper function ƒë·ªÉ l·∫•y context manager"""
    return ModelContext(model_manager)

