import pyaudio
import wave
import time
import numpy as np
from datetime import datetime
import os

# MFCC imports
try:
    import librosa
    import librosa.feature
    from scipy.signal import butter, filtfilt
    LIBROSA_AVAILABLE = True
except ImportError:
    LIBROSA_AVAILABLE = False
    print("‚ö†Ô∏è librosa kh√¥ng c√≥ s·∫µn. C√†i ƒë·∫∑t b·∫±ng: pip install librosa")

try:
    from fastdtw import fastdtw
    from scipy.spatial.distance import euclidean
    DTW_AVAILABLE = True
except ImportError:
    DTW_AVAILABLE = False

def record_audio():
    """
    Thu √¢m 20 gi√¢y t·ª´ c·ªïng line-in v√† l∆∞u th√†nh file WAV
    """
    # Thi·∫øt l·∫≠p c√°c th√¥ng s·ªë thu √¢m
    CHUNK = 1024  # K√≠ch th∆∞·ªõc buffer
    FORMAT = pyaudio.paInt16  # 16-bit audio
    CHANNELS = 2  # Stereo
    RATE = 44100  # Sample rate 44.1kHz
    RECORD_SECONDS = 20  # Thu √¢m 20 gi√¢y
    
    # T·∫°o t√™n file v·ªõi timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    WAVE_OUTPUT_FILENAME = f"recorded_audio_{timestamp}.wav"
    
    # Kh·ªüi t·∫°o PyAudio
    p = pyaudio.PyAudio()
    
    try:
        print("B·∫Øt ƒë·∫ßu thu √¢m...")
        print("ƒêang thu √¢m trong 20 gi√¢y...")
        
        # M·ªü stream ƒë·ªÉ thu √¢m
        stream = p.open(format=FORMAT,
                       channels=CHANNELS,
                       rate=RATE,
                       input=True,
                       frames_per_buffer=CHUNK)
        
        frames = []
        
        # Thu √¢m trong 20 gi√¢y
        for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
            data = stream.read(CHUNK)
            frames.append(data)
            
            # Hi·ªÉn th·ªã ti·∫øn tr√¨nh
            if i % (int(RATE / CHUNK)) == 0:
                remaining = RECORD_SECONDS - (i // int(RATE / CHUNK))
                print(f"C√≤n l·∫°i: {remaining} gi√¢y...")
        
        print("Ho√†n th√†nh thu √¢m!")
        
        # ƒê√≥ng stream
        stream.stop_stream()
        stream.close()
        
        # L∆∞u file WAV
        print(f"ƒêang l∆∞u file: {WAVE_OUTPUT_FILENAME}")
        wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(p.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))
        wf.close()
        
        print(f"File √¢m thanh ƒë√£ ƒë∆∞·ª£c l∆∞u: {WAVE_OUTPUT_FILENAME}")
        
    except Exception as e:
        print(f"L·ªói khi thu √¢m: {e}")
        print("Ki·ªÉm tra:")
        print("1. Microphone/Line-in c√≥ ƒë∆∞·ª£c k·∫øt n·ªëi kh√¥ng?")
        print("2. C√≥ c√†i ƒë·∫∑t pyaudio kh√¥ng? (pip install pyaudio)")
        
    finally:
        # ƒê√≥ng PyAudio
        p.terminate()

def read_wav_file(filename):
    """
    ƒê·ªçc file WAV v√† tr·∫£ v·ªÅ d·ªØ li·ªáu √¢m thanh
    """
    try:
        with wave.open(filename, 'rb') as wf:
            frames = wf.readframes(wf.getnframes())
            audio_data = np.frombuffer(frames, dtype=np.int16)
            
            # N·∫øu l√† stereo, chuy·ªÉn th√†nh mono b·∫±ng c√°ch l·∫•y trung b√¨nh
            if wf.getnchannels() == 2:
                audio_data = audio_data.reshape(-1, 2)
                audio_data = np.mean(audio_data, axis=1)
            
            return audio_data, wf.getframerate()
    except Exception as e:
        print(f"L·ªói ƒë·ªçc file {filename}: {e}")
        return None, None

def resample_and_normalize_audio(audio_data, current_rate, target_rate=44100):
    """
    Resample v√† normalize audio data
    """
    # Convert to mono if stereo
    if len(audio_data.shape) > 1:
        audio_data = np.mean(audio_data, axis=1)
    
    # Simple resampling (linear interpolation)
    if current_rate != target_rate:
        duration = len(audio_data) / current_rate
        new_length = int(duration * target_rate)
        audio_data = np.interp(
            np.linspace(0, len(audio_data), new_length),
            np.arange(len(audio_data)),
            audio_data
        )
    
    # Convert to float and normalize
    audio_data = audio_data.astype(np.float64)
    audio_data = audio_data - np.mean(audio_data)  # Remove DC offset
    
    # Normalize volume
    if np.std(audio_data) > 0:
        audio_data = audio_data / np.std(audio_data)
    
    return audio_data, target_rate

def extract_noise_profile(audio_data, sample_rate, noise_duration=2.0):
    """
    Extract noise profile t·ª´ ƒëo·∫°n ƒë·∫ßu file (ch·ªâ c√≥ "eee...eee")
    """
    noise_samples = int(noise_duration * sample_rate)
    if len(audio_data) < noise_samples:
        noise_samples = len(audio_data) // 2
    
    noise_segment = audio_data[:noise_samples]
    
    # T√≠nh power spectrum c·ªßa noise
    noise_stft = librosa.stft(noise_segment, n_fft=2048, hop_length=512)
    noise_magnitude = np.abs(noise_stft)
    noise_power = noise_magnitude ** 2
    
    # Average power spectrum (noise profile)
    noise_profile = np.mean(noise_power, axis=1)
    
    print(f"   üîß Noise profile extracted t·ª´ {noise_duration}s ƒë·∫ßu ({noise_samples} samples)")
    return noise_profile

def spectral_subtraction(audio_data, noise_profile, sample_rate, alpha=2.0, beta=0.01):
    """
    Spectral subtraction s·ª≠ d·ª•ng noise profile ∆∞·ªõc t√≠nh
    """
    try:
        # STFT of input signal
        stft = librosa.stft(audio_data, n_fft=2048, hop_length=512)
        magnitude = np.abs(stft)
        phase = np.angle(stft)
        power = magnitude ** 2
        
        # Expand noise profile to match STFT frames
        noise_power_expanded = noise_profile[:, np.newaxis]
        
        # Spectral subtraction
        clean_power = power - alpha * noise_power_expanded
        
        # Ensure non-negative power v·ªõi floor (beta * original power)
        floor = beta * power
        clean_power = np.maximum(clean_power, floor)
        
        # Reconstruct magnitude v√† combine v·ªõi original phase
        clean_magnitude = np.sqrt(clean_power)
        clean_stft = clean_magnitude * np.exp(1j * phase)
        
        # ISTFT to get clean audio
        clean_audio = librosa.istft(clean_stft, hop_length=512)
        
        print(f"   üîß Spectral subtraction applied (Œ±={alpha}, Œ≤={beta})")
        return clean_audio
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Spectral subtraction failed: {e}")
        return audio_data

def bandpass_filter_speech(audio_data, sample_rate, low_freq=100, high_freq=3800):
    """
    Bandpass filter ƒë·ªÉ ch·ªâ gi·ªØ speech frequency range
    """
    try:
        # Thi·∫øt k·∫ø bandpass filter cho speech (100-3800Hz)
        nyquist = sample_rate / 2
        low = low_freq / nyquist
        high = high_freq / nyquist
        
        # Butterworth bandpass filter
        b, a = butter(4, [low, high], btype='band')
        filtered_audio = filtfilt(b, a, audio_data)
        
        print(f"   üîß Bandpass filter: {low_freq}-{high_freq}Hz (speech range)")
        
        return filtered_audio
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Bandpass filter failed: {e}, s·ª≠ d·ª•ng audio g·ªëc")
        return audio_data

def professional_preprocessing(audio_data, sample_rate, target_sr=16000, 
                              noise_duration=2.0, apply_noise_reduction=True):
    """
    Professional preprocessing pipeline theo ƒë√∫ng chu·∫©n speech processing
    """
    processed_audio = audio_data.copy()
    
    # 1. Resample v·ªÅ 16kHz (optimal cho speech)
    if sample_rate != target_sr:
        processed_audio = librosa.resample(processed_audio, orig_sr=sample_rate, target_sr=target_sr)
        print(f"   üîß Resampled: {sample_rate}Hz ‚Üí {target_sr}Hz")
        sample_rate = target_sr
    
    # 2. Amplitude normalization
    if np.max(np.abs(processed_audio)) > 0:
        processed_audio = processed_audio / np.max(np.abs(processed_audio))
    
    # 3. Pre-emphasis filter (0.97) - quan tr·ªçng cho speech
    processed_audio = librosa.effects.preemphasis(processed_audio, coef=0.97)
    print(f"   üîß Pre-emphasis applied (coef=0.97)")
    
    if apply_noise_reduction:
        # 4. Extract noise profile t·ª´ ƒëo·∫°n ƒë·∫ßu
        noise_profile = extract_noise_profile(processed_audio, sample_rate, noise_duration)
        
        # 5. Spectral subtraction
        processed_audio = spectral_subtraction(processed_audio, noise_profile, sample_rate)
        
        # 6. Bandpass filter 100-3800Hz (optimal cho speech)
        processed_audio = bandpass_filter_speech(processed_audio, sample_rate, 
                                               low_freq=100, high_freq=3800)
    
    return processed_audio, sample_rate

def professional_mfcc_extraction(audio_data, sample_rate, apply_noise_reduction=True):
    """
    Professional MFCC extraction theo chu·∫©n c√¥ng nghi·ªáp
    - 20 MFCC coefficients
    - 25ms frame, 10ms hop
    - Delta + Delta-Delta features  
    - CMVN normalization
    """
    if not LIBROSA_AVAILABLE:
        raise ImportError("librosa is required for professional MFCC extraction")
    
    try:
        # 1. Professional preprocessing
        processed_audio, final_sr = professional_preprocessing(
            audio_data, sample_rate, apply_noise_reduction=apply_noise_reduction
        )
        
        # 2. Optimal MFCC parameters cho speech
        frame_length = int(0.025 * final_sr)  # 25ms frame
        hop_length = int(0.01 * final_sr)     # 10ms hop
        n_mfcc = 20                           # 20 coefficients
        
        print(f"   üìä MFCC params: {n_mfcc} coeffs, {frame_length} frame, {hop_length} hop")
        
        # 3. Extract MFCC features
        mfcc = librosa.feature.mfcc(
            y=processed_audio.astype(np.float32),
            sr=final_sr,
            n_mfcc=n_mfcc,
            n_fft=frame_length * 2,  # Th∆∞·ªùng g·∫•p ƒë√¥i frame length
            hop_length=hop_length,
            window='hamming',
            center=True
        )
        
        # 4. Delta features (t·ªëc ƒë·ªô thay ƒë·ªïi)
        delta_mfcc = librosa.feature.delta(mfcc, order=1)
        
        # 5. Delta-Delta features (gia t·ªëc)
        delta2_mfcc = librosa.feature.delta(mfcc, order=2)
        
        # 6. Combine all features
        all_features = np.vstack([mfcc, delta_mfcc, delta2_mfcc])  # Shape: (60, time_frames)
        
        # 7. CMVN normalization (c·ª±c k·ª≥ quan tr·ªçng cho noise robustness!)
        # Cepstral Mean and Variance Normalization
        mean_norm = np.mean(all_features, axis=1, keepdims=True)
        std_norm = np.std(all_features, axis=1, keepdims=True) + 1e-8  # Tr√°nh division by zero
        
        cmvn_features = (all_features - mean_norm) / std_norm
        
        # 8. Transpose ƒë·ªÉ c√≥ shape (time_frames, feature_dims)
        final_features = cmvn_features.T
        
        print(f"   üìä Professional MFCC shape: {final_features.shape}")
        print(f"       ‚îú‚îÄ MFCC: {n_mfcc} coeffs")
        print(f"       ‚îú‚îÄ Delta: {n_mfcc} coeffs") 
        print(f"       ‚îú‚îÄ Delta¬≤: {n_mfcc} coeffs")
        print(f"       ‚îî‚îÄ CMVN normalized: ‚úÖ")
        
        return final_features
        
    except Exception as e:
        print(f"   ‚ùå Professional MFCC extraction failed: {e}")
        return None

def compute_dtw_distance(mfcc1, mfcc2):
    """
    T√≠nh kho·∫£ng c√°ch DTW gi·ªØa 2 chu·ªói MFCC
    """
    if not DTW_AVAILABLE:
        # Fallback: s·ª≠ d·ª•ng Euclidean distance ƒë∆°n gi·∫£n
        min_len = min(len(mfcc1), len(mfcc2))
        mfcc1_crop = mfcc1[:min_len]
        mfcc2_crop = mfcc2[:min_len]
        
        distances = [euclidean(f1, f2) for f1, f2 in zip(mfcc1_crop, mfcc2_crop)]
        return np.mean(distances)
    
    try:
        # S·ª≠ d·ª•ng FastDTW v·ªõi euclidean distance
        distance, path = fastdtw(mfcc1, mfcc2, dist=euclidean)
        
        # Normalize distance b·ªüi chi·ªÅu d√†i chu·ªói
        normalized_distance = distance / len(path)
        
        return normalized_distance
        
    except Exception as e:
        print(f"L·ªói khi t√≠nh DTW: {e}")
        # Fallback
        min_len = min(len(mfcc1), len(mfcc2))
        mfcc1_crop = mfcc1[:min_len]
        mfcc2_crop = mfcc2[:min_len]
        distances = [euclidean(f1, f2) for f1, f2 in zip(mfcc1_crop, mfcc2_crop)]
        return np.mean(distances)

def mfcc_sliding_window_match(template_3s, check_file_audio, sample_rate, distance_threshold=15.0, use_enhanced=True):
    """
    Tr∆∞·ª£t c·ª≠a s·ªï 3s template (MFCC) qua file check ƒë·ªÉ t√¨m match t·ªët nh·∫•t
    S·ª≠ d·ª•ng Professional MFCC + DTW distance v·ªõi noise reduction
    """
    if not LIBROSA_AVAILABLE:
        print("‚ùå C·∫ßn c√†i ƒë·∫∑t librosa ƒë·ªÉ s·ª≠ d·ª•ng MFCC!")
        return float('inf'), 0
    
    window_size = len(template_3s)  # 3s template
    step_size = sample_rate // 10  # 0.1s step
    
    best_distance = float('inf')
    best_position = 0
    total_comparisons = 0
    valid_comparisons = 0
    
    method_name = "Professional MFCC Pipeline"
    print(f"üéµ {method_name} Sliding Window (3s template)...")
    print(f"   Template length: {len(template_3s)} samples")
    print(f"   Check file length: {len(check_file_audio)} samples")
    print(f"   Distance threshold: {distance_threshold}")
    
    # Extract MFCC cho template
    print(f"   üîß Extracting {method_name} cho template...")
    
    template_mfcc = professional_mfcc_extraction(template_3s, sample_rate, apply_noise_reduction=True)
    
    if template_mfcc is None:
        print("   ‚ùå Kh√¥ng th·ªÉ extract MFCC cho template!")
        return float('inf'), 0
    
    print(f"   üìä Template MFCC shape: {template_mfcc.shape}")
    
    # Scan v·ªõi sliding window
    for i in range(0, len(check_file_audio) - window_size + 1, step_size):
        total_comparisons += 1
        
        # L·∫•y ƒëo·∫°n 3s t·ª´ file check
        check_segment = check_file_audio[i:i+window_size]
        
        # Skip n·∫øu segment qu√° y√™n tƒ©nh
        segment_rms = np.sqrt(np.mean(check_segment**2))
        if segment_rms < 0.1:  # Threshold cho normalized data
            continue
        
        # Extract MFCC cho segment
        check_mfcc = professional_mfcc_extraction(check_segment, sample_rate, apply_noise_reduction=True)
        
        if check_mfcc is None:
            continue
            
        valid_comparisons += 1
        
        # T√≠nh DTW distance
        try:
            dtw_distance = compute_dtw_distance(template_mfcc, check_mfcc)
            
            if dtw_distance < best_distance:
                best_distance = dtw_distance
                best_position = i / sample_rate
                
                if dtw_distance < distance_threshold * 1.5:  # Show progress
                    print(f"   üìà Good match: DTW={dtw_distance:.2f} at {best_position:.1f}s")
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error at position {i/sample_rate:.1f}s: {e}")
            continue
    
    print(f"   ‚úÖ Scan complete. Best DTW distance: {best_distance:.2f} at {best_position:.1f}s")
    print(f"   üìä Stats: {total_comparisons} total, {valid_comparisons} valid")
    
    return best_distance, best_position

def find_best_template_mfcc_match_enhanced(check_file, distance_threshold=15.0, skip_seconds=0):
    """
    Streamlined version - auto s·ª≠ d·ª•ng Professional MFCC pipeline
    Ch·ªâ check 2 templates ch√≠nh: THUE BAO v√† SO KHONG DUNG
    """
    if not LIBROSA_AVAILABLE:
        print("‚ùå C·∫ßn c√†i ƒë·∫∑t librosa ƒë·ªÉ s·ª≠ d·ª•ng MFCC!")
        print("   Ch·∫°y: pip install librosa")
        return None
    
    print(f"\nüéµ T√åM TEMPLATE KH·ªöP NH·∫§T (Professional MFCC + DTW)")
    print("üîß Professional Pipeline: 16kHz+Pre-emphasis+Noise profile+CMVN")
    print(f"File c·∫ßn check: {check_file}")
    print("="*60)
    
    # Ch·ªâ check 2 templates ch√≠nh
    templates = {
        "THUE BAO": "template_thue_bao_ok.wav",
        "SO KHONG DUNG": "template_so_khong_dung_ok.wav"
    }
    
    print(f"üìã Templates: {', '.join(templates.keys())}")
    
    # ƒê·ªçc file c·∫ßn check
    check_audio, check_rate = read_wav_file(check_file)
    if check_audio is None:
        print(f"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc file: {check_file}")
        return None
    
    print(f"üìÇ File check: {len(check_audio)} samples, {check_rate}Hz")
    
    # Chu·∫©n b·ªã file check
    check_audio_norm, target_rate = resample_and_normalize_audio(check_audio, check_rate)
    
    # Skip N gi√¢y ƒë·∫ßu n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
    if skip_seconds > 0:
        skip_samples = int(skip_seconds * target_rate)
        if skip_samples < len(check_audio_norm):
            check_audio_norm = check_audio_norm[skip_samples:]
            print(f"   ‚úÇÔ∏è ƒê√£ b·ªè qua {skip_seconds}s ƒë·∫ßu ({skip_samples} samples)")
            print(f"   Sau skip: {len(check_audio_norm)} samples, {target_rate}Hz")
        else:
            print(f"   ‚ö†Ô∏è Kh√¥ng th·ªÉ skip {skip_seconds}s - file qu√° ng·∫Øn!")
    else:
        print(f"   Sau normalize: {len(check_audio_norm)} samples, {target_rate}Hz")
    
    results = {}
    
    # Ki·ªÉm tra t·ª´ng template 
    for template_name, template_file in templates.items():
        if not os.path.exists(template_file):
            print(f"\n‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y: {template_file}")
            continue
            
        print(f"\nüéµ KI·ªÇM TRA TEMPLATE: {template_name}")
        print("-" * 40)
        
        # ƒê·ªçc template
        template_audio, template_rate = read_wav_file(template_file)
        if template_audio is None:
            print(f"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc template: {template_file}")
            continue
        
        print(f"üìÇ Template: {len(template_audio)} samples, {template_rate}Hz")
        
        # Chu·∫©n b·ªã template (resample + normalize)
        template_norm, _ = resample_and_normalize_audio(template_audio, template_rate, target_rate)
        
        # DEBUG: Template stats
        orig_rms = np.sqrt(np.mean(template_norm**2))
        print(f"   üìä Template RMS sau normalize: {orig_rms:.3f}")
        
        # L·∫•y 3s ƒë·∫ßu template
        window_3s = target_rate * 3
        template_3s = template_norm[:min(window_3s, len(template_norm))]
        
        actual_duration = len(template_3s) / target_rate
        print(f"   ‚úÇÔ∏è S·ª≠ d·ª•ng {actual_duration:.1f}s ƒë·∫ßu template ({len(template_3s)} samples)")
        
        # MFCC sliding window match
        distance, position = mfcc_sliding_window_match(
            template_3s, check_audio_norm, target_rate, distance_threshold, use_enhanced=True
        )
        
        # Convert distance to similarity score v·ªõi adaptive scaling
        if distance == float('inf'):
            similarity_score = 0.0
        else:
            # Adaptive scaling d·ª±a tr√™n enhanced features
            if distance < 8.0:      # Excellent match
                similarity_score = 0.95 + (8.0 - distance) * 0.005  # 0.95-1.0
            elif distance < 15.0:   # Good match  
                similarity_score = 0.8 + (15.0 - distance) * 0.015 / 7.0  # 0.8-0.95
            elif distance < 25.0:   # Fair match
                similarity_score = 0.5 + (25.0 - distance) * 0.3 / 10.0   # 0.5-0.8
            else:                   # Poor match
                similarity_score = max(0, 0.5 - (distance - 25.0) * 0.02)  # 0-0.5
        
        results[template_name] = {
            'distance': distance,
            'similarity': similarity_score,
            'position': position,
        }
        
        print(f"   üéØ K·∫øt qu·∫£: DTW distance={distance:.2f}, similarity={similarity_score:.3f} (t·∫°i {position:.1f}s)")
    
    # T·ªïng k·∫øt k·∫øt qu·∫£
    print("\n" + "="*60)
    print(f"üìä T·ªîNG K·∫æT K·∫æT QU·∫¢ (Professional MFCC + DTW):")
    print("="*60)
    
    if not results:
        print("‚ùå Kh√¥ng c√≥ template n√†o ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng")
        return None
    
    # S·∫Øp x·∫øp theo distance (th·∫•p nh·∫•t = t·ªët nh·∫•t)
    sorted_results = sorted(results.items(), key=lambda x: x[1]['distance'])
    
    for i, (name, data) in enumerate(sorted_results):
        icon = "ü•á" if i == 0 else "ü•à"
        distance = data['distance']
        similarity = data['similarity']
        if distance == float('inf'):
            print(f"{icon} {name}: DTW=‚àû (kh√¥ng t√≠nh ƒë∆∞·ª£c)")
        else:
            print(f"{icon} {name}: DTW={distance:.2f} Sim={similarity:.3f} (t·∫°i {data['position']:.1f}s)")
    
    # X√°c ƒë·ªãnh k·∫øt qu·∫£ cu·ªëi c√πng
    best_template, best_data = sorted_results[0]
    best_distance = best_data['distance']
    best_similarity = best_data['similarity']
    
    print(f"\nüéØ TEMPLATE KH·ªöP NH·∫§T: {best_template}")
    print(f"üìà DTW Distance: {best_distance:.2f}")
    print(f"üìà Similarity Score: {best_similarity:.3f} ({best_similarity*100:.1f}%)")
    print(f"üìç V·ªã tr√≠ kh·ªõp: {best_data['position']:.1f}s")
    
    # Logic: DTW > 9.3 th√¨ tr·∫£ v·ªÅ HOAT DONG
    dtw_threshold = 9.3
    
    # ƒê√°nh gi√° ƒë·ªô tin c·∫≠y
    if best_distance == float('inf'):
        print(f"‚ùå KH√îNG TH·ªÇ T√çNH TO√ÅN - L·ªói MFCC extraction")
        final_result = "ERROR - MFCC_FAILED"
    elif best_distance <= dtw_threshold:
        print(f"‚úÖ K·∫æT QU·∫¢ TIN C·∫¨Y (DTW ‚â§ {dtw_threshold})")
        final_result = best_template
    else:
        print(f"‚ö†Ô∏è DTW > {dtw_threshold} ‚Üí PH√ÇN LO·∫†I L√Ä HO·∫†T ƒê·ªòNG")
        final_result = "HOAT DONG"
    
    print("="*60)
    print(f"\nüéØ K·∫æT QU·∫¢ CU·ªêI C√ôNG: {final_result}")
    return final_result

def main_menu():
    """
    Menu ch√≠nh - Ch·ªâ Option 1 v√† 7
    """
    # Ki·ªÉm tra template files
    templates = [
        "template_thue_bao_ok.wav",
        "template_so_khong_dung_ok.wav"
    ]
    
    print("=== THU √ÇM V√Ä SO S√ÅNH √ÇM THANH ===")
    print("1. Thu √¢m m·ªõi")
    print("7. üéµ T√¨m template kh·ªõp nh·∫•t (Professional MFCC + DTW)")
    print("0. Tho√°t")
    
    choice = input("\nCh·ªçn ch·ª©c nƒÉng (1, 7, 0): ").strip()
    
    if choice == "1":
        record_audio()
        
    elif choice == "7":
        # Ki·ªÉm tra template files
        missing_templates = [t for t in templates if not os.path.exists(t)]
        if missing_templates:
            print("‚ö†Ô∏è Thi·∫øu c√°c template files:")
            for template in missing_templates:
                print(f"   - {template}")
            print("Vui l√≤ng ƒë·∫£m b·∫£o c√≥ ƒë·ªß 2 template files!")
            return
        
        # Ki·ªÉm tra dependencies
        if not LIBROSA_AVAILABLE:
            print("‚ùå Thi·∫øu th∆∞ vi·ªán librosa!")
            print("   C√†i ƒë·∫∑t b·∫±ng: pip install librosa")
            return
        
        if not DTW_AVAILABLE:
            print("‚ö†Ô∏è Thi·∫øu th∆∞ vi·ªán fastdtw (s·∫Ω d√πng fallback method)")
            print("   ƒê·ªÉ c√≥ hi·ªáu su·∫•t t·ªët nh·∫•t, c√†i ƒë·∫∑t: pip install fastdtw")
        
        print("üéµ T√åM TEMPLATE KH·ªöP NH·∫§T (Professional MFCC + DTW)")
        print("Ph∆∞∆°ng ph√°p: Professional speech processing pipeline + DTW distance")
        print("üìã Template files:")
        print("   - THUE BAO: template_thue_bao_ok.wav")
        print("   - SO KHONG DUNG: template_so_khong_dung_ok.wav") 
        print("\nüîß PROFESSIONAL PIPELINE (t·ª± ƒë·ªông):")
        print("   ‚úÖ Resample 16kHz + Pre-emphasis (0.97)")
        print("   ‚úÖ Noise profile estimation t·ª´ ƒëo·∫°n ƒë·∫ßu")
        print("   ‚úÖ Spectral subtraction + Bandpass 100-3800Hz")
        print("   ‚úÖ 20 MFCC + Delta + Delta¬≤ (60 features)")
        print("   ‚úÖ CMVN normalization (noise robust)")
        print("   ‚úÖ DTW matching v·ªõi optimal parameters")
        print("   ‚úÖ Frame 25ms, Hop 10ms (chu·∫©n speech)")
        
        # Nh·∫≠p t√™n file c·∫ßn ki·ªÉm tra
        input_filename = input("\nNh·∫≠p t√™n file √¢m thanh c·∫ßn check (VD: recorded_audio.wav): ").strip()
        
        # Ki·ªÉm tra file t·ªìn t·∫°i
        if not os.path.exists(input_filename):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {input_filename}")
            print("Vui l√≤ng ki·ªÉm tra l·∫°i t√™n file v√† ƒë∆∞·ªùng d·∫´n!")
            return
        
        print(f"‚úÖ T√¨m th·∫•y file: {input_filename}")
        
        # Hi·ªÉn th·ªã th√¥ng tin file
        try:
            audio_data, sample_rate = read_wav_file(input_filename)
            if audio_data is not None:
                duration = len(audio_data) / sample_rate
                rms_value = np.sqrt(np.mean(audio_data**2))
                print(f"üìä Th√¥ng tin file:")
                print(f"   - ƒê·ªô d√†i: {duration:.1f} gi√¢y")
                print(f"   - Sample rate: {sample_rate} Hz")
                print(f"   - RMS: {rms_value:.1f}")
        except:
            pass
        
        print(f"\nüöÄ B·∫Øt ƒë·∫ßu Professional MFCC + DTW analysis...")
        
        input("\nNh·∫•n Enter ƒë·ªÉ b·∫Øt ƒë·∫ßu analysis...")
        
        # T√¨m template kh·ªõp nh·∫•t v·ªõi MFCC + DTW
        result = find_best_template_mfcc_match_enhanced(input_filename)
        
        if result:
            print(f"\nüéØ K·∫æT QU·∫¢ CU·ªêI C√ôNG: {result}")
        else:
            print(f"\n‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω file ho·∫∑c thi·∫øu th∆∞ vi·ªán c·∫ßn thi·∫øt")
            
    elif choice == "0":
        print("T·∫°m bi·ªát!")
        return
    else:
        print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")
    
    # Quay l·∫°i menu
    input("\nNh·∫•n Enter ƒë·ªÉ ti·∫øp t·ª•c...")
    main_menu()

if __name__ == "__main__":
    main_menu()
