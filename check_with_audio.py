import pyaudio
import wave
import time
import numpy as np
from datetime import datetime
import os

# MFCC imports
try:
    import librosa
    import librosa.feature
    from scipy.signal import butter, filtfilt
    LIBROSA_AVAILABLE = True
except ImportError:
    LIBROSA_AVAILABLE = False
    print("âš ï¸ librosa khÃ´ng cÃ³ sáºµn. CÃ i Ä‘áº·t báº±ng: pip install librosa")

try:
    from fastdtw import fastdtw
    from scipy.spatial.distance import euclidean
    DTW_AVAILABLE = True
except ImportError:
    DTW_AVAILABLE = False

def record_audio():
    """
    Thu Ã¢m 20 giÃ¢y tá»« cá»•ng line-in vÃ  lÆ°u thÃ nh file WAV
    """
    # Thiáº¿t láº­p cÃ¡c thÃ´ng sá»‘ thu Ã¢m
    CHUNK = 1024  # KÃ­ch thÆ°á»›c buffer
    FORMAT = pyaudio.paInt16  # 16-bit audio
    CHANNELS = 2  # Stereo
    RATE = 44100  # Sample rate 44.1kHz
    RECORD_SECONDS = 20  # Thu Ã¢m 20 giÃ¢y
    
    # Táº¡o tÃªn file vá»›i timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    WAVE_OUTPUT_FILENAME = f"recorded_audio_{timestamp}.wav"
    
    # Khá»Ÿi táº¡o PyAudio
    p = pyaudio.PyAudio()
    
    try:
        print("Báº¯t Ä‘áº§u thu Ã¢m...")
        print("Äang thu Ã¢m trong 20 giÃ¢y...")
        
        # Má»Ÿ stream Ä‘á»ƒ thu Ã¢m
        stream = p.open(format=FORMAT,
                       channels=CHANNELS,
                       rate=RATE,
                       input=True,
                       frames_per_buffer=CHUNK)
        
        frames = []
        
        # Thu Ã¢m trong 20 giÃ¢y
        for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
            data = stream.read(CHUNK)
            frames.append(data)
            
            # Hiá»ƒn thá»‹ tiáº¿n trÃ¬nh
            if i % (int(RATE / CHUNK)) == 0:
                remaining = RECORD_SECONDS - (i // int(RATE / CHUNK))
                print(f"CÃ²n láº¡i: {remaining} giÃ¢y...")
        
        print("HoÃ n thÃ nh thu Ã¢m!")
        
        # ÄÃ³ng stream
        stream.stop_stream()
        stream.close()
        
        # LÆ°u file WAV
        print(f"Äang lÆ°u file: {WAVE_OUTPUT_FILENAME}")
        wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(p.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))
        wf.close()
        
        print(f"File Ã¢m thanh Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {WAVE_OUTPUT_FILENAME}")
        
    except Exception as e:
        print(f"Lá»—i khi thu Ã¢m: {e}")
        print("Kiá»ƒm tra:")
        print("1. Microphone/Line-in cÃ³ Ä‘Æ°á»£c káº¿t ná»‘i khÃ´ng?")
        print("2. CÃ³ cÃ i Ä‘áº·t pyaudio khÃ´ng? (pip install pyaudio)")
        
    finally:
        # ÄÃ³ng PyAudio
        p.terminate()

def read_wav_file(filename):
    """
    Äá»c file WAV vÃ  tráº£ vá» dá»¯ liá»‡u Ã¢m thanh
    """
    try:
        with wave.open(filename, 'rb') as wf:
            frames = wf.readframes(wf.getnframes())
            audio_data = np.frombuffer(frames, dtype=np.int16)
            
            # Náº¿u lÃ  stereo, chuyá»ƒn thÃ nh mono báº±ng cÃ¡ch láº¥y trung bÃ¬nh
            if wf.getnchannels() == 2:
                audio_data = audio_data.reshape(-1, 2)
                audio_data = np.mean(audio_data, axis=1)
            
            return audio_data, wf.getframerate()
    except Exception as e:
        print(f"Lá»—i Ä‘á»c file {filename}: {e}")
        return None, None

def resample_and_normalize_audio(audio_data, current_rate, target_rate=44100):
    """
    Resample vÃ  normalize audio data
    """
    # Convert to mono if stereo
    if len(audio_data.shape) > 1:
        audio_data = np.mean(audio_data, axis=1)
    
    # Simple resampling (linear interpolation)
    if current_rate != target_rate:
        duration = len(audio_data) / current_rate
        new_length = int(duration * target_rate)
        audio_data = np.interp(
            np.linspace(0, len(audio_data), new_length),
            np.arange(len(audio_data)),
            audio_data
        )
    
    # Convert to float and normalize
    audio_data = audio_data.astype(np.float64)
    audio_data = audio_data - np.mean(audio_data)  # Remove DC offset
    
    # Normalize volume
    if np.std(audio_data) > 0:
        audio_data = audio_data / np.std(audio_data)
    
    return audio_data, target_rate

def extract_noise_profile(audio_data, sample_rate, noise_duration=2.0):
    """
    Extract noise profile tá»« Ä‘oáº¡n Ä‘áº§u file (chá»‰ cÃ³ "eee...eee")
    """
    noise_samples = int(noise_duration * sample_rate)
    if len(audio_data) < noise_samples:
        noise_samples = len(audio_data) // 2
    
    noise_segment = audio_data[:noise_samples]
    
    # TÃ­nh power spectrum cá»§a noise
    noise_stft = librosa.stft(noise_segment, n_fft=2048, hop_length=512)
    noise_magnitude = np.abs(noise_stft)
    noise_power = noise_magnitude ** 2
    
    # Average power spectrum (noise profile)
    noise_profile = np.mean(noise_power, axis=1)
    
    print(f"   ğŸ”§ Noise profile extracted tá»« {noise_duration}s Ä‘áº§u ({noise_samples} samples)")
    return noise_profile

def spectral_subtraction(audio_data, noise_profile, sample_rate, alpha=2.0, beta=0.01):
    """
    Spectral subtraction sá»­ dá»¥ng noise profile Æ°á»›c tÃ­nh
    """
    try:
        # STFT of input signal
        stft = librosa.stft(audio_data, n_fft=2048, hop_length=512)
        magnitude = np.abs(stft)
        phase = np.angle(stft)
        power = magnitude ** 2
        
        # Expand noise profile to match STFT frames
        noise_power_expanded = noise_profile[:, np.newaxis]
        
        # Spectral subtraction
        clean_power = power - alpha * noise_power_expanded
        
        # Ensure non-negative power vá»›i floor (beta * original power)
        floor = beta * power
        clean_power = np.maximum(clean_power, floor)
        
        # Reconstruct magnitude vÃ  combine vá»›i original phase
        clean_magnitude = np.sqrt(clean_power)
        clean_stft = clean_magnitude * np.exp(1j * phase)
        
        # ISTFT to get clean audio
        clean_audio = librosa.istft(clean_stft, hop_length=512)
        
        print(f"   ğŸ”§ Spectral subtraction applied (Î±={alpha}, Î²={beta})")
        return clean_audio
        
    except Exception as e:
        print(f"   âš ï¸ Spectral subtraction failed: {e}")
        return audio_data

def bandpass_filter_speech(audio_data, sample_rate, low_freq=100, high_freq=3800):
    """
    Bandpass filter Ä‘á»ƒ chá»‰ giá»¯ speech frequency range
    """
    try:
        # Thiáº¿t káº¿ bandpass filter cho speech (100-3800Hz)
        nyquist = sample_rate / 2
        low = low_freq / nyquist
        high = high_freq / nyquist
        
        # Butterworth bandpass filter
        b, a = butter(4, [low, high], btype='band')
        filtered_audio = filtfilt(b, a, audio_data)
        
        print(f"   ğŸ”§ Bandpass filter: {low_freq}-{high_freq}Hz (speech range)")
        
        return filtered_audio
        
    except Exception as e:
        print(f"   âš ï¸ Bandpass filter failed: {e}, sá»­ dá»¥ng audio gá»‘c")
        return audio_data

def professional_preprocessing(audio_data, sample_rate, target_sr=16000, 
                              noise_duration=2.0, apply_noise_reduction=True):
    """
    Professional preprocessing pipeline theo Ä‘Ãºng chuáº©n speech processing
    """
    processed_audio = audio_data.copy()
    
    # 1. Resample vá» 16kHz (optimal cho speech)
    if sample_rate != target_sr:
        processed_audio = librosa.resample(processed_audio, orig_sr=sample_rate, target_sr=target_sr)
        print(f"   ğŸ”§ Resampled: {sample_rate}Hz â†’ {target_sr}Hz")
        sample_rate = target_sr
    
    # 2. Amplitude normalization
    if np.max(np.abs(processed_audio)) > 0:
        processed_audio = processed_audio / np.max(np.abs(processed_audio))
    
    # 3. Pre-emphasis filter (0.97) - quan trá»ng cho speech
    processed_audio = librosa.effects.preemphasis(processed_audio, coef=0.97)
    print(f"   ğŸ”§ Pre-emphasis applied (coef=0.97)")
    
    if apply_noise_reduction:
        # 4. Extract noise profile tá»« Ä‘oáº¡n Ä‘áº§u
        noise_profile = extract_noise_profile(processed_audio, sample_rate, noise_duration)
        
        # 5. Spectral subtraction
        processed_audio = spectral_subtraction(processed_audio, noise_profile, sample_rate)
        
        # 6. Bandpass filter 100-3800Hz (optimal cho speech)
        processed_audio = bandpass_filter_speech(processed_audio, sample_rate, 
                                               low_freq=100, high_freq=3800)
    
    return processed_audio, sample_rate

def professional_mfcc_extraction(audio_data, sample_rate, apply_noise_reduction=True):
    """
    Professional MFCC extraction theo chuáº©n cÃ´ng nghiá»‡p
    - 20 MFCC coefficients
    - 25ms frame, 10ms hop
    - Delta + Delta-Delta features  
    - CMVN normalization
    """
    if not LIBROSA_AVAILABLE:
        raise ImportError("librosa is required for professional MFCC extraction")
    
    try:
        # 1. Professional preprocessing
        processed_audio, final_sr = professional_preprocessing(
            audio_data, sample_rate, apply_noise_reduction=apply_noise_reduction
        )
        
        # 2. Optimal MFCC parameters cho speech
        frame_length = int(0.025 * final_sr)  # 25ms frame
        hop_length = int(0.01 * final_sr)     # 10ms hop
        n_mfcc = 20                           # 20 coefficients
        
        print(f"   ğŸ“Š MFCC params: {n_mfcc} coeffs, {frame_length} frame, {hop_length} hop")
        
        # 3. Extract MFCC features
        mfcc = librosa.feature.mfcc(
            y=processed_audio.astype(np.float32),
            sr=final_sr,
            n_mfcc=n_mfcc,
            n_fft=frame_length * 2,  # ThÆ°á»ng gáº¥p Ä‘Ã´i frame length
            hop_length=hop_length,
            window='hamming',
            center=True
        )
        
        # 4. Delta features (tá»‘c Ä‘á»™ thay Ä‘á»•i)
        delta_mfcc = librosa.feature.delta(mfcc, order=1)
        
        # 5. Delta-Delta features (gia tá»‘c)
        delta2_mfcc = librosa.feature.delta(mfcc, order=2)
        
        # 6. Combine all features
        all_features = np.vstack([mfcc, delta_mfcc, delta2_mfcc])  # Shape: (60, time_frames)
        
        # 7. CMVN normalization (cá»±c ká»³ quan trá»ng cho noise robustness!)
        # Cepstral Mean and Variance Normalization
        mean_norm = np.mean(all_features, axis=1, keepdims=True)
        std_norm = np.std(all_features, axis=1, keepdims=True) + 1e-8  # TrÃ¡nh division by zero
        
        cmvn_features = (all_features - mean_norm) / std_norm
        
        # 8. Transpose Ä‘á»ƒ cÃ³ shape (time_frames, feature_dims)
        final_features = cmvn_features.T
        
        print(f"   ğŸ“Š Professional MFCC shape: {final_features.shape}")
        print(f"       â”œâ”€ MFCC: {n_mfcc} coeffs")
        print(f"       â”œâ”€ Delta: {n_mfcc} coeffs") 
        print(f"       â”œâ”€ DeltaÂ²: {n_mfcc} coeffs")
        print(f"       â””â”€ CMVN normalized: âœ…")
        
        return final_features
        
    except Exception as e:
        print(f"   âŒ Professional MFCC extraction failed: {e}")
        return None

def compute_dtw_distance(mfcc1, mfcc2):
    """
    TÃ­nh khoáº£ng cÃ¡ch DTW giá»¯a 2 chuá»—i MFCC
    """
    if not DTW_AVAILABLE:
        # Fallback: sá»­ dá»¥ng Euclidean distance Ä‘Æ¡n giáº£n
        min_len = min(len(mfcc1), len(mfcc2))
        mfcc1_crop = mfcc1[:min_len]
        mfcc2_crop = mfcc2[:min_len]
        
        distances = [euclidean(f1, f2) for f1, f2 in zip(mfcc1_crop, mfcc2_crop)]
        return np.mean(distances)
    
    try:
        # Sá»­ dá»¥ng FastDTW vá»›i euclidean distance
        distance, path = fastdtw(mfcc1, mfcc2, dist=euclidean)
        
        # Normalize distance bá»Ÿi chiá»u dÃ i chuá»—i
        normalized_distance = distance / len(path)
        
        return normalized_distance
        
    except Exception as e:
        print(f"Lá»—i khi tÃ­nh DTW: {e}")
        # Fallback
        min_len = min(len(mfcc1), len(mfcc2))
        mfcc1_crop = mfcc1[:min_len]
        mfcc2_crop = mfcc2[:min_len]
        distances = [euclidean(f1, f2) for f1, f2 in zip(mfcc1_crop, mfcc2_crop)]
        return np.mean(distances)

def mfcc_sliding_window_match(template_3s, check_file_audio, sample_rate, distance_threshold=15.0, use_enhanced=True):
    """
    TrÆ°á»£t cá»­a sá»• 3s template (MFCC) qua file check Ä‘á»ƒ tÃ¬m match tá»‘t nháº¥t
    Sá»­ dá»¥ng Professional MFCC + DTW distance vá»›i noise reduction
    """
    if not LIBROSA_AVAILABLE:
        print("âŒ Cáº§n cÃ i Ä‘áº·t librosa Ä‘á»ƒ sá»­ dá»¥ng MFCC!")
        return float('inf'), 0
    
    window_size = len(template_3s)  # 3s template
    step_size = sample_rate // 10  # 0.1s step
    
    best_distance = float('inf')
    best_position = 0
    total_comparisons = 0
    valid_comparisons = 0
    
    method_name = "Professional MFCC Pipeline"
    print(f"ğŸµ {method_name} Sliding Window (3s template)...")
    print(f"   Template length: {len(template_3s)} samples")
    print(f"   Check file length: {len(check_file_audio)} samples")
    print(f"   Distance threshold: {distance_threshold}")
    
    # Extract MFCC cho template
    print(f"   ğŸ”§ Extracting {method_name} cho template...")
    
    template_mfcc = professional_mfcc_extraction(template_3s, sample_rate, apply_noise_reduction=True)
    
    if template_mfcc is None:
        print("   âŒ KhÃ´ng thá»ƒ extract MFCC cho template!")
        return float('inf'), 0
    
    print(f"   ğŸ“Š Template MFCC shape: {template_mfcc.shape}")
    
    # Scan vá»›i sliding window
    for i in range(0, len(check_file_audio) - window_size + 1, step_size):
        total_comparisons += 1
        
        # Láº¥y Ä‘oáº¡n 3s tá»« file check
        check_segment = check_file_audio[i:i+window_size]
        
        # Skip náº¿u segment quÃ¡ yÃªn tÄ©nh
        segment_rms = np.sqrt(np.mean(check_segment**2))
        if segment_rms < 0.1:  # Threshold cho normalized data
            continue
        
        # Extract MFCC cho segment
        check_mfcc = professional_mfcc_extraction(check_segment, sample_rate, apply_noise_reduction=True)
        
        if check_mfcc is None:
            continue
            
        valid_comparisons += 1
        
        # TÃ­nh DTW distance
        try:
            dtw_distance = compute_dtw_distance(template_mfcc, check_mfcc)
            
            if dtw_distance < best_distance:
                best_distance = dtw_distance
                best_position = i / sample_rate
                
                if dtw_distance < distance_threshold * 1.5:  # Show progress
                    print(f"   ğŸ“ˆ Good match: DTW={dtw_distance:.2f} at {best_position:.1f}s")
                
        except Exception as e:
            print(f"   âš ï¸ Error at position {i/sample_rate:.1f}s: {e}")
            continue
    
    print(f"   âœ… Scan complete. Best DTW distance: {best_distance:.2f} at {best_position:.1f}s")
    print(f"   ğŸ“Š Stats: {total_comparisons} total, {valid_comparisons} valid")
    
    return best_distance, best_position

def find_best_template_mfcc_match_enhanced(check_file, distance_threshold=15.0, skip_seconds=0):
    """
    Streamlined version - auto sá»­ dá»¥ng Professional MFCC pipeline
    Chá»‰ check 2 templates chÃ­nh: THUE BAO vÃ  SO KHONG DUNG
    """
    if not LIBROSA_AVAILABLE:
        print("âŒ Cáº§n cÃ i Ä‘áº·t librosa Ä‘á»ƒ sá»­ dá»¥ng MFCC!")
        print("   Cháº¡y: pip install librosa")
        return None
    
    print(f"\nğŸµ TÃŒM TEMPLATE KHá»šP NHáº¤T (Professional MFCC + DTW)")
    print("ğŸ”§ Professional Pipeline: 16kHz+Pre-emphasis+Noise profile+CMVN")
    print(f"File cáº§n check: {check_file}")
    print("="*60)
    
    # Chá»‰ check 2 templates chÃ­nh
    templates = {
        "THUE BAO": "template_thue_bao_ok.wav",
        "SO KHONG DUNG": "template_so_khong_dung_ok.wav"
    }
    
    print(f"ğŸ“‹ Templates: {', '.join(templates.keys())}")
    
    # Äá»c file cáº§n check
    check_audio, check_rate = read_wav_file(check_file)
    if check_audio is None:
        print(f"âŒ KhÃ´ng thá»ƒ Ä‘á»c file: {check_file}")
        return None
    
    print(f"ğŸ“‚ File check: {len(check_audio)} samples, {check_rate}Hz")
    
    # Chuáº©n bá»‹ file check
    check_audio_norm, target_rate = resample_and_normalize_audio(check_audio, check_rate)
    
    # Skip N giÃ¢y Ä‘áº§u náº¿u Ä‘Æ°á»£c yÃªu cáº§u
    if skip_seconds > 0:
        skip_samples = int(skip_seconds * target_rate)
        if skip_samples < len(check_audio_norm):
            check_audio_norm = check_audio_norm[skip_samples:]
            print(f"   âœ‚ï¸ ÄÃ£ bá» qua {skip_seconds}s Ä‘áº§u ({skip_samples} samples)")
            print(f"   Sau skip: {len(check_audio_norm)} samples, {target_rate}Hz")
        else:
            print(f"   âš ï¸ KhÃ´ng thá»ƒ skip {skip_seconds}s - file quÃ¡ ngáº¯n!")
    else:
        print(f"   Sau normalize: {len(check_audio_norm)} samples, {target_rate}Hz")
    
    results = {}
    
    # Kiá»ƒm tra tá»«ng template 
    for template_name, template_file in templates.items():
        if not os.path.exists(template_file):
            print(f"\nâš ï¸ KhÃ´ng tÃ¬m tháº¥y: {template_file}")
            continue
            
        print(f"\nğŸµ KIá»‚M TRA TEMPLATE: {template_name}")
        print("-" * 40)
        
        # Äá»c template
        template_audio, template_rate = read_wav_file(template_file)
        if template_audio is None:
            print(f"âŒ KhÃ´ng thá»ƒ Ä‘á»c template: {template_file}")
            continue
        
        print(f"ğŸ“‚ Template: {len(template_audio)} samples, {template_rate}Hz")
        
        # Chuáº©n bá»‹ template (resample + normalize)
        template_norm, _ = resample_and_normalize_audio(template_audio, template_rate, target_rate)
        
        # DEBUG: Template stats
        orig_rms = np.sqrt(np.mean(template_norm**2))
        print(f"   ğŸ“Š Template RMS sau normalize: {orig_rms:.3f}")
        
        # Láº¥y 3s Ä‘áº§u template
        window_3s = target_rate * 3
        template_3s = template_norm[:min(window_3s, len(template_norm))]
        
        actual_duration = len(template_3s) / target_rate
        print(f"   âœ‚ï¸ Sá»­ dá»¥ng {actual_duration:.1f}s Ä‘áº§u template ({len(template_3s)} samples)")
        
        # MFCC sliding window match
        distance, position = mfcc_sliding_window_match(
            template_3s, check_audio_norm, target_rate, distance_threshold, use_enhanced=True
        )
        
        # Convert distance to similarity score vá»›i adaptive scaling
        if distance == float('inf'):
            similarity_score = 0.0
        else:
            # Adaptive scaling dá»±a trÃªn enhanced features
            if distance < 8.0:      # Excellent match
                similarity_score = 0.95 + (8.0 - distance) * 0.005  # 0.95-1.0
            elif distance < 15.0:   # Good match  
                similarity_score = 0.8 + (15.0 - distance) * 0.015 / 7.0  # 0.8-0.95
            elif distance < 25.0:   # Fair match
                similarity_score = 0.5 + (25.0 - distance) * 0.3 / 10.0   # 0.5-0.8
            else:                   # Poor match
                similarity_score = max(0, 0.5 - (distance - 25.0) * 0.02)  # 0-0.5
        
        results[template_name] = {
            'distance': distance,
            'similarity': similarity_score,
            'position': position,
        }
        
        print(f"   ğŸ¯ Káº¿t quáº£: DTW distance={distance:.2f}, similarity={similarity_score:.3f} (táº¡i {position:.1f}s)")
    
    # Tá»•ng káº¿t káº¿t quáº£
    print("\n" + "="*60)
    print(f"ğŸ“Š Tá»”NG Káº¾T Káº¾T QUáº¢ (Professional MFCC + DTW):")
    print("="*60)
    
    if not results:
        print("âŒ KhÃ´ng cÃ³ template nÃ o Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng")
        return None
    
    # Sáº¯p xáº¿p theo distance (tháº¥p nháº¥t = tá»‘t nháº¥t)
    sorted_results = sorted(results.items(), key=lambda x: x[1]['distance'])
    
    for i, (name, data) in enumerate(sorted_results):
        icon = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ"
        distance = data['distance']
        similarity = data['similarity']
        if distance == float('inf'):
            print(f"{icon} {name}: DTW=âˆ (khÃ´ng tÃ­nh Ä‘Æ°á»£c)")
        else:
            print(f"{icon} {name}: DTW={distance:.2f} Sim={similarity:.3f} (táº¡i {data['position']:.1f}s)")
    
    # XÃ¡c Ä‘á»‹nh káº¿t quáº£ cuá»‘i cÃ¹ng
    best_template, best_data = sorted_results[0]
    best_distance = best_data['distance']
    best_similarity = best_data['similarity']
    
    print(f"\nğŸ¯ TEMPLATE KHá»šP NHáº¤T: {best_template}")
    print(f"ğŸ“ˆ DTW Distance: {best_distance:.2f}")
    print(f"ğŸ“ˆ Similarity Score: {best_similarity:.3f} ({best_similarity*100:.1f}%)")
    print(f"ğŸ“ Vá»‹ trÃ­ khá»›p: {best_data['position']:.1f}s")
    
    # Logic: DTW > 9.3 thÃ¬ tráº£ vá» HOAT DONG
    dtw_threshold = 9.3
    
    # ÄÃ¡nh giÃ¡ Ä‘á»™ tin cáº­y
    if best_distance == float('inf'):
        print(f"âŒ KHÃ”NG THá»‚ TÃNH TOÃN - Lá»—i MFCC extraction")
        final_result = "ERROR - MFCC_FAILED"
    elif best_distance <= dtw_threshold:
        print(f"âœ… Káº¾T QUáº¢ TIN Cáº¬Y (DTW â‰¤ {dtw_threshold})")
        final_result = best_template
    else:
        print(f"âš ï¸ DTW > {dtw_threshold} â†’ PHÃ‚N LOáº I LÃ€ HOáº T Äá»˜NG")
        final_result = "HOAT DONG"
    
    print("="*60)
    print(f"\nğŸ¯ Káº¾T QUáº¢ CUá»I CÃ™NG: {final_result}")
    return final_result

def main_menu():
    """
    Menu chÃ­nh - Chá»‰ Option 1 vÃ  7
    """
    # Kiá»ƒm tra template files
    templates = [
        "template_thue_bao_ok.wav",
        "template_so_khong_dung_ok.wav"
    ]
    
    print("=== THU Ã‚M VÃ€ SO SÃNH Ã‚M THANH ===")
    print("1. Thu Ã¢m má»›i")
    print("7. ğŸµ TÃ¬m template khá»›p nháº¥t (Professional MFCC + DTW)")
    print("0. ThoÃ¡t")
    
    choice = input("\nChá»n chá»©c nÄƒng (1, 7, 0): ").strip()
    
    if choice == "1":
        record_audio()
        
    elif choice == "7":
        # Kiá»ƒm tra template files
        missing_templates = [t for t in templates if not os.path.exists(t)]
        if missing_templates:
            print("âš ï¸ Thiáº¿u cÃ¡c template files:")
            for template in missing_templates:
                print(f"   - {template}")
            print("Vui lÃ²ng Ä‘áº£m báº£o cÃ³ Ä‘á»§ 2 template files!")
            return
        
        # Kiá»ƒm tra dependencies
        if not LIBROSA_AVAILABLE:
            print("âŒ Thiáº¿u thÆ° viá»‡n librosa!")
            print("   CÃ i Ä‘áº·t báº±ng: pip install librosa")
            return
        
        if not DTW_AVAILABLE:
            print("âš ï¸ Thiáº¿u thÆ° viá»‡n fastdtw (sáº½ dÃ¹ng fallback method)")
            print("   Äá»ƒ cÃ³ hiá»‡u suáº¥t tá»‘t nháº¥t, cÃ i Ä‘áº·t: pip install fastdtw")
        
        print("ğŸµ TÃŒM TEMPLATE KHá»šP NHáº¤T (Professional MFCC + DTW)")
        print("PhÆ°Æ¡ng phÃ¡p: Professional speech processing pipeline + DTW distance")
        print("ğŸ“‹ Template files:")
        print("   - THUE BAO: template_thue_bao_ok.wav")
        print("   - SO KHONG DUNG: template_so_khong_dung_ok.wav") 
        print("\nğŸ”§ PROFESSIONAL PIPELINE (tá»± Ä‘á»™ng):")
        print("   âœ… Resample 16kHz + Pre-emphasis (0.97)")
        print("   âœ… Noise profile estimation tá»« Ä‘oáº¡n Ä‘áº§u")
        print("   âœ… Spectral subtraction + Bandpass 100-3800Hz")
        print("   âœ… 20 MFCC + Delta + DeltaÂ² (60 features)")
        print("   âœ… CMVN normalization (noise robust)")
        print("   âœ… DTW matching vá»›i optimal parameters")
        print("   âœ… Frame 25ms, Hop 10ms (chuáº©n speech)")
        
        # Nháº­p tÃªn file cáº§n kiá»ƒm tra
        input_filename = input("\nNháº­p tÃªn file Ã¢m thanh cáº§n check (VD: recorded_audio.wav): ").strip()
        
        # Kiá»ƒm tra file tá»“n táº¡i
        if not os.path.exists(input_filename):
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {input_filename}")
            print("Vui lÃ²ng kiá»ƒm tra láº¡i tÃªn file vÃ  Ä‘Æ°á»ng dáº«n!")
            return
        
        print(f"âœ… TÃ¬m tháº¥y file: {input_filename}")
        
        # Hiá»ƒn thá»‹ thÃ´ng tin file
        try:
            audio_data, sample_rate = read_wav_file(input_filename)
            if audio_data is not None:
                duration = len(audio_data) / sample_rate
                rms_value = np.sqrt(np.mean(audio_data**2))
                print(f"ğŸ“Š ThÃ´ng tin file:")
                print(f"   - Äá»™ dÃ i: {duration:.1f} giÃ¢y")
                print(f"   - Sample rate: {sample_rate} Hz")
                print(f"   - RMS: {rms_value:.1f}")
        except:
            pass
        
        print(f"\nğŸš€ Báº¯t Ä‘áº§u Professional MFCC + DTW analysis...")
        
        input("\nNháº¥n Enter Ä‘á»ƒ báº¯t Ä‘áº§u analysis...")
        
        # TÃ¬m template khá»›p nháº¥t vá»›i MFCC + DTW
        result = find_best_template_mfcc_match_enhanced(input_filename)
        
        if result:
            print(f"\nğŸ¯ Káº¾T QUáº¢ CUá»I CÃ™NG: {result}")
        else:
            print(f"\nâŒ KhÃ´ng thá»ƒ xá»­ lÃ½ file hoáº·c thiáº¿u thÆ° viá»‡n cáº§n thiáº¿t")
            
    elif choice == "0":
        print("Táº¡m biá»‡t!")
        return
    else:
        print("Lá»±a chá»n khÃ´ng há»£p lá»‡!")
    
    # Quay láº¡i menu
    input("\nNháº¥n Enter Ä‘á»ƒ tiáº¿p tá»¥c...")
    main_menu()

if __name__ == "__main__":
    main_menu()
